# ğŸ“˜ **Spark Â· RDD Â· DataFrame Â· Structured Streaming Â· Kafka â€” ì´ë¡  ì´ì •ë¦¬**

---

# # ğŸ§© **1. Apache Sparkë€?**

### ğŸ’¡ **SparkëŠ” ëŒ€ê·œëª¨ ë°ì´í„°ë¥¼ "ë¶„ì‚° ì²˜ë¦¬"í•˜ê¸° ìœ„í•œ í´ëŸ¬ìŠ¤í„° ì»´í“¨íŒ… ì—”ì§„**

## âœ¨ Sparkì˜ í•µì‹¬ íŠ¹ì§•

* **In-memory ì—°ì‚°**: Hadoop MapReduceëŠ” ë””ìŠ¤í¬ ê¸°ë°˜ â†’ SparkëŠ” ë©”ëª¨ë¦¬ ê¸°ë°˜ì´ë¼ 100ë°°ê¹Œì§€ ë¹ ë¦„
* **Lazy Evaluation(ì§€ì—° ì‹¤í–‰)**: ë³€í™˜(Transformation)ì€ ì¦‰ì‹œ ì‹¤í–‰ë˜ì§€ ì•Šê³  DAG í˜•íƒœë¡œ ìŒ“ì˜€ë‹¤ê°€ Actionì´ í˜¸ì¶œë  ë•Œ ì‹¤í–‰ë¨
* **Fault-tolerance**: RDDì˜ Lineage(ì—°ì‚° ì´ë ¥) ê¸°ë°˜ìœ¼ë¡œ ë°ì´í„° ë³µêµ¬ ê°€ëŠ¥
* **ë‹¤ì–‘í•œ API ì œê³µ**

  * **RDD API**
  * **DataFrame API**
  * **SQL API**
  * **Structured Streaming**

---

# # ğŸ§© **2. RDD (Resilient Distributed Dataset)**

### RDDëŠ” Sparkì˜ **ê°€ì¥ ê¸°ë³¸ì ì¸ ë¶„ì‚° ë°ì´í„° ëª¨ë¸**

## ğŸ”¥ RDDì˜ íŠ¹ì§•

* **Immutable(ë¶ˆë³€)**: í•œ ë²ˆ ìƒì„±ë˜ë©´ ë³€ê²½ë˜ì§€ ì•ŠìŒ (ìƒˆ ê°ì²´ë¥¼ ìƒì„±)
* **Distributed(ë¶„ì‚°)**: í´ëŸ¬ìŠ¤í„°ì˜ ì—¬ëŸ¬ ë…¸ë“œì— ë‚˜ëˆ„ì–´ ì €ì¥ë¨
* **Fault-tolerant**: Lineageë¥¼ í†µí•´ ìë™ ë³µêµ¬ ê°€ëŠ¥
* **Lazy Evaluation** ì ìš©

---

## ğŸ“Œ Transformation vs Action

| ë¶„ë¥˜                 | íŠ¹ì§•                | ì˜ˆì‹œ                                  |
| ------------------ | ----------------- | ----------------------------------- |
| **Transformation** | ì‹¤í–‰ë˜ì§€ ì•Šê³  DAGì— ê¸°ë¡ë¨  | map, filter, flatMap, mapPartitions |
| **Action**         | ì‹¤ì œ í´ëŸ¬ìŠ¤í„°ì—ì„œ ì—°ì‚°ì´ ì‹¤í–‰ë¨ | collect, count, take, reduce        |

---

## ğŸ“Œ ì£¼ìš” Transformation

### â‘  **map**

* ìš”ì†Œ 1ê°œ â†’ 1ê°œë¡œ ë³€í™˜í•˜ëŠ” í•¨ìˆ˜

```python
rdd.map(lambda x: x * 2)
```

### â‘¡ **filter**

* ì¡°ê±´ì„ ë§Œì¡±í•˜ëŠ” ìš”ì†Œë§Œ í†µê³¼

```python
rdd.filter(lambda x: x % 2 == 0)
```

### â‘¢ **flatMap**

* ìš”ì†Œ 1ê°œ â†’ ì—¬ëŸ¬ ê°œë¡œ í™•ì¥í•  ë•Œ ì‚¬ìš©

```python
rdd.flatMap(lambda x: x.split(" "))
```

### â‘£ **mapPartitions**

* íŒŒí‹°ì…˜ ë‹¨ìœ„ë¡œ ë°ì´í„°ë¥¼ ì²˜ë¦¬
* ë§µë³´ë‹¤ í›¨ì”¬ ë¹ ë¦„ (íŒŒí‹°ì…˜ ë‹¨ìœ„ë¡œ Python â†” JVM ì˜¤ë²„í—¤ë“œ ê°ì†Œ)

```python
rdd.mapPartitions(lambda iter: (x*2 for x in iter))
```

---

## ğŸ“Œ ì£¼ìš” Action

### â‘  collect()

RDDë¥¼ ëª¨ë‘ ë“œë¼ì´ë²„ë¡œ ê°€ì ¸ì˜´
â†’ ë°ì´í„°ê°€ í´ ê²½ìš° ì ˆëŒ€ ì‚¬ìš©í•˜ë©´ ì•ˆ ë¨

### â‘¡ count()

ìš”ì†Œ ê°œìˆ˜ ë°˜í™˜

### â‘¢ take(n)

ì•ì—ì„œ nê°œ ë°ì´í„°ë§Œ ìˆ˜ì§‘

### â‘£ reduce()

ì§‘ê³„ ì—°ì‚° ìˆ˜í–‰

---

# # ğŸ§© **3. RDD Sampling & randomSplit**

### âœ” sample(withReplacement, fraction)

```python
rdd.sample(False, 0.2)  # ë¹„ë³µì›, 20%
```

### âœ” takeSample

```python
rdd.takeSample(False, 5)
```

### âœ” randomSplit

í›ˆë ¨Â·í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¶„í• í•  ë•Œ ì‚¬ìš©

```python
train, test = rdd.randomSplit([0.8, 0.2], seed=42)
```

---

# # ğŸ§© **4. DataFrame & Spark SQL**

### DataFrameì€ RDDë³´ë‹¤ ë” ê³ ìˆ˜ì¤€ API

â†’ ìŠ¤í‚¤ë§ˆ ê¸°ë°˜, Catalyst Optimizer ì‚¬ìš© â†’ í›¨ì”¬ ë¹ ë¥´ê³  íš¨ìœ¨ì 

---

## ğŸ“Œ DataFrame ìƒì„±

```python
df = spark.createDataFrame(data, columns)
```

## ğŸ“Œ select, filter, orderBy, groupBy

```python
df.select("name", "age")
df.filter(col("age") > 25)
df.orderBy(col("age").desc())
df.groupBy("department").agg(avg("age"))
```

---

## ğŸ“Œ ìŠ¤í‚¤ë§ˆ ì¶œë ¥

```python
df.printSchema()
```

---

## ğŸ“Œ ìŠ¤í‚¤ë§ˆ ì •ì˜(StructType)

```python
schema = StructType([
    StructField("Name", StringType()),
    StructField("Age", IntegerType())
])
```

---

## ğŸ“Œ Type Casting (ë°ì´í„° íƒ€ì… ë³€í™˜)

### ë¬¸ìì—´ ìˆ«ìë¥¼ ì •ìˆ˜í˜•ìœ¼ë¡œ ë³€í™˜

```python
df.withColumn("Age", col("Age").cast(IntegerType()))
```

### ë¬¸ìì—´ ë‚ ì§œë¥¼ Date íƒ€ì…ìœ¼ë¡œ ë³€í™˜

```python
df.withColumn("Order_Date", to_date(col("OrderDate"), "dd-MM-yyyy"))
```

---

# # ğŸ§© **5. Spark Structured Streaming**

### ğŸ’¡ Spark Structured Streamingì€ *ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë° ì²˜ë¦¬ í”„ë ˆì„ì›Œí¬*

---

## âš™ï¸ Structured Streamingì˜ í•µì‹¬ ê°œë…

### â‘  Micro-batch Model

* ì‹¤ì‹œê°„ ë°ì´í„°ì§€ë§Œ ë‚´ë¶€ì ìœ¼ë¡œëŠ” **ë§ˆì´í¬ë¡œ ë°°ì¹˜ ë‹¨ìœ„ë¡œ ì²˜ë¦¬**

### â‘¡ Event Time vs Processing Time

| ì¢…ë¥˜                  | ì˜ë¯¸                 |
| ------------------- | ------------------ |
| **Event Time**      | ë°ì´í„°ê°€ ì‹¤ì œ ë°œìƒí•œ ì‹œê°„     |
| **Processing Time** | Sparkê°€ ë°ì´í„°ë¥¼ ì²˜ë¦¬í•œ ì‹œê°„ |

ì‹œí—˜ì— ìì£¼ ë‚˜ì˜¤ëŠ” ë‚´ìš©

---

### â‘¢ Watermark (ì§€ì—° ì´ë²¤íŠ¸ í—ˆìš©)

ì§€ì—°ëœ ë°ì´í„°ê°€ ëŠ¦ê²Œ ë“¤ì–´ì˜¬ ê²ƒì„ í—ˆìš©í•˜ëŠ” ì‹œê°„ ì„¤ì •

```python
.withWatermark("timestamp", "2 minutes")
```

---

### â‘£ Window Function

1ë¶„ ë‹¨ìœ„ë¡œ ë¬¶ì–´ ì§‘ê³„

```python
groupBy(window(col("timestamp"), "1 minute"))
```

---

# # ğŸ§© **6. Kafka + Spark Structured Streaming**

Kafka â†’ Sparkë¡œ ë°ì´í„°ê°€ ë“¤ì–´ì˜¤ëŠ” ì „ì²´ íë¦„

```
[Kafka Producer] â†’ [Kafka Broker] â†’ [Spark Structured Streaming] â†’ [Console / DB / File]
```

---

## ğŸ“Œ Kafka Producer

* JSON ë©”ì‹œì§€ë¥¼ Kafka topicìœ¼ë¡œ ë°œí–‰(send)

```python
producer.send("click-events", json)
```

---

## ğŸ“Œ Sparkì—ì„œ Kafka ì½ê¸°

```python
df = spark.readStream \
    .format("kafka") \
    .option("subscribe", "click-events") \
    .load()
```

---

## ğŸ“Œ Kafka value íŒŒì‹±

Kafka valueëŠ” **ë°”ì´ë„ˆë¦¬** â†’ ë¬¸ìì—´ ë³€í™˜ í•„ìš”

```python
value_df = df.selectExpr("CAST(value AS STRING)")
```

---

## ğŸ“Œ JSON íŒŒì‹± (from_json)

```python
parsed_df = value_df.select(
    from_json(col("value"), schema).alias("data")
).select("data.*")
```

---

## ğŸ“Œ Window + GroupBy ì§‘ê³„

```python
result_df = parsed_df \
    .withWatermark("timestamp", "2 minutes") \
    .groupBy(
        window(col("timestamp"), "1 minute"),
        col("event_type")
    ) \
    .agg(
        count("*").alias("event_count"),
        approx_count_distinct("user_id").alias("unique_users")
    )
```

â€» ì‹œí—˜ í¬ì¸íŠ¸ â†’ ìŠ¤íŠ¸ë¦¬ë°ì—ì„œëŠ”
`countDistinct` ëŒ€ì‹  **`approx_count_distinct`** ì‚¬ìš©í•´ì•¼ í•¨ (ì •í™•í•œ ì¹´ìš´íŠ¸ ë¶ˆê°€)

---

## ğŸ“Œ ê²°ê³¼ ì¶œë ¥ (writeStream)

```python
result_df.writeStream \
    .outputMode("complete") \
    .format("console") \
    .start() \
    .awaitTermination()
```

---

# # ğŸ§© **7. Kafka ì„œë²„ êµ¬ì„± íë¦„**

### â‘  Zookeeper ì‹¤í–‰

```
bin/zookeeper-server-start.sh config/zookeeper.properties
```

### â‘¡ Kafka Broker ì‹¤í–‰

```
bin/kafka-server-start.sh config/server.properties
```

### â‘¢ Producer ì‹¤í–‰

```
python kafka_producer.py
```

### â‘£ Spark Streaming ì‹¤í–‰

```
python streaming_job.py
```

---

# # ğŸ§© **8. ì‹œí—˜ì— ì˜ ë‚˜ì˜¤ëŠ” ê°œë… ì •ë¦¬ (ì•”ê¸° í•„ìˆ˜)**

### ğŸ”¥ Spark ê°œë…

* In-memory processing
* Lazy evaluation
* DAG (Directed Acyclic Graph)
* Transformation vs Action

### ğŸ”¥ RDD

* Immutable
* Fault-tolerance (Lineage)
* map / filter / flatMap / mapPartitions
* sample / randomSplit

### ğŸ”¥ DataFrame

* ìŠ¤í‚¤ë§ˆ ê¸°ë°˜ êµ¬ì¡° ë°ì´í„°
* Catalyst Optimizer
* DataFrame API vs SQL API

### ğŸ”¥ Structured Streaming

* Micro-batch
* Event time / processing time
* Watermark
* Window function
* outputMode("append" / "complete" / "update")

### ğŸ”¥ Kafka + Spark

* Kafka valueëŠ” Binary
  â†’ CAST(value AS STRING) í•„ìˆ˜
* from_json()ìœ¼ë¡œ íŒŒì‹±
* ìŠ¤íŠ¸ë¦¬ë°ì—ì„œëŠ” approx_count_distinct ì‚¬ìš©

